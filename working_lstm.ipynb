{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as sdf\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import alpaca_trade_api as tradeapi\n",
    "from pathlib import Path\n",
    "import lib_copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Python-dotenv could not parse statement starting at line 7\nPython-dotenv could not parse statement starting at line 8\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = lib_copy.fetch_ohlcv(\"JPM\", \"2017-01-01\", \"2019-01-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_boll_kelt_ewma_dataframe(dataframe):\n",
    "    lib_copy.bollinger_band_generator(dataframe)\n",
    "    lib_copy.keltner_channel(dataframe)\n",
    "    lib_copy.ewma(dataframe)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything = adding_boll_kelt_ewma_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for missing values\n",
    "everything.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with the previous ones\n",
    "everything = df.dropna()\n",
    "everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data = lib_copy.signals_generator(everything)\n",
    "\n",
    "response_data.index = response_data.index.date\n",
    "final_df = response_data\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"practice logic\n",
    "\n",
    "merge actual cross and actual squeeze DF's\n",
    "carolina's_df = pd.Concatenate(ewma_cross, squeeze)\n",
    "\n",
    "feed merged df into jonathans target function using 1+1 = 2 target mechanism\n",
    "\n",
    "use lstm model: utilizing the squeeze and the ewma cross as features and the target as the target\n",
    "\n",
    "spit out graph\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.iloc[:, 0:20].values\n",
    "y = final_df.iloc[:, :1].values\n",
    "\n",
    "X, y = np.array(X), np.array(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f\"X sample values:\\n{X[:3]} \\n\")\n",
    "print (f\"X sample values:\\n{y[:3]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually splitting the data\n",
    "split = int(0.7 * len(X))\n",
    "\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the MinMaxScaler from sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the MinMaxScaler object with the features data X\n",
    "scaler.fit(X)\n",
    "\n",
    "# Scale the features training and testing sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit the MinMaxScaler object with the target data Y\n",
    "scaler.fit(y)\n",
    "\n",
    "# Scale the target training and testing sets\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape the features data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Print some sample data after reshaping the datasets\n",
    "print (f\"X_train sample values:\\n{X_train[:3]} \\n\")\n",
    "print (f\"X_test sample values:\\n{X_test[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Keras modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM RNN model.\n",
    "model = Sequential()\n",
    "\n",
    "# Initial model setup\n",
    "number_units = 30\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 3\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, shuffle=False, batch_size=90, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing data X_test\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the original prices instead of the scaled version\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of Real and Predicted values\n",
    "stocks = pd.DataFrame({\n",
    "    \"Actual\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "}, index = final_df.index[-len(real_prices): ]) \n",
    "\n",
    "# Show the DataFrame's head\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the real vs predicted prices as a line chart\n",
    "stocks.plot(title=\"Actual Vs. Predicted  Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.DataFrame(stocks)\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_ema = 9\n",
    "slow_ema = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create EMAs columns\n",
    "price['pEMA9'] = price['Predicted'].ewm(span=fast_ema, adjust=False).mean()\n",
    "price['pEMA21'] = price['Predicted'].ewm(span=slow_ema, adjust=False).mean()\n",
    "price['aEMA9'] = price['Actual'].ewm(span=fast_ema, adjust=False).mean()\n",
    "price['aEMA21'] = price['Actual'].ewm(span=slow_ema, adjust=False).mean()\n",
    "# price['compEMAS'] = 0.0\n",
    "price.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.pct_change().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as sdf\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import alpaca_trade_api as tradeapi\n",
    "from pathlib import Path\n",
    "import lib_copy\n",
    "load_dotenv()\n",
    "%matplotlib inline\n",
    "\n",
    "ticker_symbol = [\"TSLA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.iloc[:, 0:20].values\n",
    "y = final_df.iloc[:, -1].values\n",
    "\n",
    "X, y = np.array(X), np.array(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = lib_copy.fetch_ohlcv(ticker_symbol)\n",
    "def adding_boll_kelt_ewma_dataframe(dataframe):\n",
    "    lib_copy.bollinger_band_generator(dataframe)\n",
    "    lib_copy.keltner_channel(dataframe)\n",
    "    lib_copy.ewma(dataframe)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "everything = adding_boll_kelt_ewma_dataframe(dataframe)\n",
    "\n",
    "everything = df.dropna()\n",
    "\n",
    "response_data = lib_copy.signals_generator(everything)\n",
    "\n",
    "response_data.index = response_data.index.date\n",
    "final_df = response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(\n",
    "    dataframe,\n",
    "    num_feature_cols = 2, \n",
    "    target_name = \"target\",\n",
    "    epochs_num = 10,\n",
    "    unit_number = 30,\n",
    "    dropout_fraction=.2    \n",
    "    ):\n",
    "\n",
    "    \"\"\"\n",
    "    Make sure to have target in last column\n",
    "    \"\"\"\n",
    "\n",
    "    X = dataframe.iloc[:, 0:num_feature_cols].values\n",
    "    \n",
    "    y = dataframe.iloc[:,-1].values\n",
    "    \n",
    "    X, y = np.array(X), np.array(y).reshape(-1,1)\n",
    "\n",
    "    # Manually splitting the data\n",
    "    split = int(0.7 * len(X))\n",
    "\n",
    "    X_train = X[: split]\n",
    "    X_test = X[split:]\n",
    "\n",
    "    y_train = y[: split]\n",
    "    y_test = y[split:]\n",
    "\n",
    "    # Importing the MinMaxScaler from sklearn\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the MinMaxScaler object with the features data X\n",
    "    scaler.fit(X)\n",
    "\n",
    "    # Scale the features training and testing sets\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Fit the MinMaxScaler object with the target data Y\n",
    "    scaler.fit(y)\n",
    "\n",
    "    # Scale the target training and testing sets\n",
    "    y_train = scaler.transform(y_train)\n",
    "    y_test = scaler.transform(y_test)\n",
    "\n",
    "    # Importing required Keras modules\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "    # Define the LSTM RNN model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(LSTM(\n",
    "        units=unit_number,\n",
    "        return_sequences=True,\n",
    "        input_shape=(X_train.shape[1], 1))\n",
    "        )\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(LSTM(units=number_units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(LSTM(units=number_units))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs= epechs_num, shuffle=False, batch_size=90, verbose=1)\n",
    "\n",
    "    # Make predictions using the testing data X_test\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    # Recover the original prices instead of the scaled version\n",
    "    predicted_prices = scaler.inverse_transform(predicted)\n",
    "    real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    # Create a DataFrame of Real and Predicted values\n",
    "    comparison = pd.DataFrame({\n",
    "        \"Actual Target\": real_prices.ravel(),\n",
    "        \"Predicted Target\": predicted_prices.ravel()\n",
    "    }, index = dataframe.index[-len(real_prices): ]) \n",
    "\n",
    "    return model.summary(), model.evaluate(X_test, y_test, verbose=0), comparison.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     squeeze  emax_signal\nDate_Time                                \n2018-01-12 08:45:00        0            0\n2018-01-12 09:00:00        0            0\n2018-01-12 09:15:00        0            0\n2018-01-12 09:30:00        0            0\n2018-01-12 09:45:00        0            0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>squeeze</th>\n      <th>emax_signal</th>\n    </tr>\n    <tr>\n      <th>Date_Time</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-01-12 08:45:00</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:00:00</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:15:00</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:30:00</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:45:00</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "beautiful_csv = pd.read_csv(\"Resources/ts_emax_x_squeeze.csv\", parse_dates=[['Date', 'Time']])\n",
    "beautiful_csv.set_index(\"Date_Time\", inplace=True)\n",
    "df = beautiful_csv[[\"squeeze\", \"emax_signal\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_generator(dataframe_name, col_name1, col_name2, target_col_name):\n",
    "    \"\"\"Creates a target for long position\n",
    "    Args:\n",
    "        dataframe_name (dict): Dataframe containing indicator data (0's and 1's)\n",
    "        col_name1 (str): Name of first column name in dataframe to use for calculation\n",
    "        col_name2 (str): Name of second column name in dataframe to use for calculation\n",
    "        target_col_name (str): Name of target column name to create and store target values\n",
    "    Returns:\n",
    "        A dataframe of:\n",
    "            original data passed to function,\n",
    "            appended target column signals of type float (2.0, 1.0, 0.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Target generation\n",
    "    for index, row in dataframe_name.iterrows():\n",
    "        dataframe_name.loc[index, target_col_name] = row[col_name1] + row[col_name2]\n",
    "\n",
    "    # Return dataframe with features and target\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                     squeeze  emax_signal  target\nDate_Time                                        \n2018-01-12 08:45:00        0            0     0.0\n2018-01-12 09:00:00        0            0     0.0\n2018-01-12 09:15:00        0            0     0.0\n2018-01-12 09:30:00        0            0     0.0\n2018-01-12 09:45:00        0            0     0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>squeeze</th>\n      <th>emax_signal</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>Date_Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-01-12 08:45:00</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:00:00</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:15:00</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:30:00</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-01-12 09:45:00</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "new_df = target_generator(df, \"squeeze\", \"emax_signal\", \"target\")\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"Resources/LSTM_target_list1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm1(\n",
    "    dataframe,\n",
    "    num_feature_cols = 2, \n",
    "    target_name = \"target\",\n",
    "    epochs_num = 10,\n",
    "    unit_number = 30,\n",
    "    dropout_fraction=.2    \n",
    "    ):\n",
    "\n",
    "    \"\"\"\n",
    "    Make sure to have target in last column\n",
    "    \"\"\"\n",
    "\n",
    "    # Manually splitting the data\n",
    "\n",
    "    # Construct training start and training end dates\n",
    "\n",
    "    training_start = dataframe.index.min().strftime(format='%Y-%m-%d')\n",
    "    training_end = '2019-01-11'\n",
    "\n",
    "    # Construct test start and test end dates\n",
    "\n",
    "    testing_start = '2019-01-12'\n",
    "    testing_end = '2019-06-12'\n",
    "\n",
    "    # Construct validating start and validating end dates\n",
    "\n",
    "    vali_start = '2019-06-13'\n",
    "    vali_end = '2020-01-12'\n",
    "\n",
    "    # Construct the X_train and y_train datasets\n",
    "    X_train = dataframe[[\"squeeze\", \"emax_signal\"]][training_start:training_end]\n",
    "    y_train = dataframe[\"target\"][training_start:training_end]\n",
    "\n",
    "    X_test = dataframe[[\"squeeze\", \"emax_signal\"]][testing_start:testing_end]\n",
    "    y_test = dataframe[\"target\"][testing_start:testing_end]\n",
    "\n",
    "    \n",
    "    X = dataframe.iloc[:, 0:num_feature_cols].values\n",
    "    \n",
    "    y = dataframe.iloc[:,-1].values\n",
    "\n",
    "    X, y = np.array(X), np.array(y).reshape(-1,1)\n",
    "    # Importing the MinMaxScaler from sklearn\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the MinMaxScaler object with the features data X\n",
    "    scaler.fit(X)\n",
    "\n",
    "    # Scale the features training and testing sets\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Fit the MinMaxScaler object with the target data Y\n",
    "    scaler.fit(y)\n",
    "\n",
    "    # Scale the target training and testing sets\n",
    "    y_train = scaler.transform(y_train)\n",
    "    y_test = scaler.transform(y_test)\n",
    "\n",
    "    # Importing required Keras modules\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "    # Define the LSTM RNN model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(LSTM(\n",
    "        units=unit_number,\n",
    "        return_sequences=True,\n",
    "        input_shape=(X_train.shape[1], 1))\n",
    "        )\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(LSTM(units=number_units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(LSTM(units=number_units))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs= epechs_num, shuffle=False, batch_size=90, verbose=1)\n",
    "\n",
    "    # Make predictions using the testing data X_test\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    # Recover the original prices instead of the scaled version\n",
    "    predicted_prices = scaler.inverse_transform(predicted)\n",
    "    real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    # Create a DataFrame of Real and Predicted values\n",
    "    comparison = pd.DataFrame({\n",
    "        \"Actual Target\": real_prices.ravel(),\n",
    "        \"Predicted Target\": predicted_prices.ravel()\n",
    "    }, index = dataframe.index[-len(real_prices): ]) \n",
    "\n",
    "    return model.summary(), model.evaluate(X_test, y_test, verbose=0), comparison.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 1. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-19c7045b1b2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_num\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_feature_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-232e0345fd8c>\u001b[0m in \u001b[0;36mlstm1\u001b[0;34m(dataframe, num_feature_cols, target_name, epochs_num, unit_number, dropout_fraction)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Scale the target training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    408\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    621\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 1. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "lstm1(new_df, epochs_num= 1, num_feature_cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1610507480344"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}